[["index.html", "PADP 7120 Homework Using R to Learn Concepts and Skills Preface", " PADP 7120 Homework Using R to Learn Concepts and Skills Alex Combs Last updated on 04 December 2023 Preface This is a collection homework assignments for students enrolled in my sections of PADP 7120: Data Applications in Public Administration. Examples and exercises are presented using R. Students who intend to use a personal computer to complete exercises in the R chapters need to download and install the following software: R RStudio Data Applications in Public Administration by Alex Combs is licensed under CC BY-NC-ND 4.0 "],["1-r-chapter-introduction.html", " 1 R Chapter Introduction 1.1 What is R and RStudio 1.2 Installing R and RStudio 1.3 RStudio orientation 1.4 Save and Upload 1.5 Additional Resources", " 1 R Chapter Introduction This section contains what are referred to as R Chapters, each of which corresponds to a chapter in the previous section. Chapters in the previous section focus on concepts that are applicable regardless of statistical software. R Chapters present those concepts in ways to practically apply them via a short series of exercises using R. Each R Chapter begins with a list of learning objectives followed by a what you need to set up in terms of packages and data to complete the chapter. Each chapter then guides you through a few exercises that require you to operate R. Periodically, they will ask you to interpret your results and/or connect what you have done to the concept it was meant to help you understand. By the end of each chapter, you will have at least one document to save and upload to eLC. That document will contain code and answers to questions. Once you upload your R Chapter work to eLC, a document will become available that contains my answers to those same exercises. This is meant to provide almost immediate feedback. You should compare your work to my own, making note of any differences and attempting to make sense of them. Keep in mind that my answers are not necessarily definitive. R Chapters will be incorporated into class discussion when possible, but feel free to ask specific questions about each R Chapter during class. 1.1 What is R and RStudio R is a programming language for statistical computing. RStudio is a user interface for R. RStudio requires R to be installed on a computer in order to function. Though you need R installed to use RStudio, you never need to launch the actual R program. Instead, you will always use RStudio for assignments in this course. 1.2 Installing R and RStudio If you intend to install R and RStudio on your computer, follow the instructions below. There are also plenty of videos online that demonstrate downloading R and RStudio if you prefer to have a visual walkthrough. First, download and install R here. Windows user: click on “Download R for Windows”, then click on “base”, then click on “Download R #.#.# for Windows.” MacOS user:, click on “Download R for (Mac) OS X.” What you click on next depends on what version of macOS is installed on your Mac and whether your Mac has an Intel chip or Apple chip. Under “Latest release,” you will see a link such as “R-4.2.2-arm64.pkg” or “R-4.2.2.pkg” with a description to the right that indicates which versions of macOS it is compatible with, such as macOS 11 (Big Sur) and higher or 10.13 (High Sierra) and higher. If you do not know which version of macOS you are using, click on the apple symbol in the top-left of your screen, then click on “About This Mac.” The resulting window will display your version of macOS. If your macOS version is compatible with the latest release, you should download one of the two options under “Latest release.” Note there are different packages to download depending on whether your machine has Apple silicon (M1 and higher) or Intel. Be sure to download the correct package. -If you are using a version of macOS older than 10.13 (High Sierra), scroll down to the header “Binaries for legacy OS X systems” where you can find the link that will work with your version. Once you have downloaded R, open the installer file (probably) located in your downloads folder. Follow the prompts to install R, similar to installing any software. Second, download and install RStudio here. Scrolling down this page, you will see “Step 1: Install R,” which you should have already completed. If so, proceed to “Step 2: Install RStudio Desktop” The blue box should provide the correct installer based on your operating system. For example, if you are using Windows OS, the blue box should display “Download RStudio Desktop for Windows.” If so, click on the link. If not, scroll down and you will see a list of all installers based on operating system (OS). The second column lists the download link. Click the link that is compatible with your OS. Once you have RStudio downloaded, open the installer file (probably) located in your downloads folder. Follow the prompts to install RStudio. 1.3 RStudio orientation Exercise 1 Launch RStudio Upon launch, you should see three sections referred to as panes: Console pane (left) is where you can tell R what to do. It also displays the results of commands and any messages, warnings, and errors. You will rarely need to use the console except when installing a package. Only install a package via the console, never as code you would save and (re)run. Environment pane (top right) displays all the data in your current R session. A session is the time between launching and closing R. Files pane (bottom right) allows you to navigate your files, displays plots, provides a list of installed packages, allows you to search for help, and displays file exports. You will usually see a fourth pane in the upper-left, with the console in the bottom-left, while working in RStudio – the source editor pane. Exercise 2 Start a new R Markdown file. You have two options to do so: 1) See that plus sign (+) icon with a piece of paper in the very top left of RStudio? Click on that and you will see a list of options. In this course, we will always use R Markdown documents. Select R Markdown. 2) In the RStudio menu bar at the top of your screen, go to File -&gt; New File -&gt; R Markdown. Either way, a dialog box will appear. You do not need to do anything other than click OK. A new pane should appear with some default content in it. This is the pane where you will tell R what to do 99% of the time because it allows you to write code that you can save. 1.3.1 R Markdown An R Markdown document allows you to fluidly combine prose that you would write for a report and R code that produces the tables and graphs you wish to incorporate. It can do much more than this, as outlined by this brief video. In this course, we will use R Markdown for R Labs and Problem Sets in addition to these R Chapters. An R Markdown file consists of two parts: YAML Header. The YAML is at the top of your R Markdown file beginning and ending with three dash marks. The YAML sets the global parameters for the document and how it exports. Below is the YAML you may see in your current R Markdown document. --- title: &quot;Untitled&quot; output: html_document --- Body: where you write prose and code. Your current R Markdown document likely opened with some content already in the body. Feel free to read this content, as it is informative. Exercise 3 In the YAML, change the title to “R Chapter 16”. Exercise 4 Click on the Knit button at the top of the source editor pane (looks like a ball of yarn with a needle stuck in it. If a drop down menu appears, click on Knit to HTML. RStudio will then prompt you to save your document. Save your R Markdown file, naming it according to the following convention (yourlastname_rch16). R will start to process your document and a new window will appear that contains your export document. You want to take some time to consider how the code and prose in the R Markdown document relates to the content you see in the exported html document. You can close this new window once you are finished viewing it. Feel free to test Knit to Word if your computer has Microsoft Word installed. Knit to PDF will not yet work. RStudio saves your R Markdown document every time you knit. 1.3.2 R Packages Many tasks in R require you to install R packages that augment its functionality. This is analogous to a smartphone that comes with base programs (e.g. calendar, weather), but others develop third-party applications to augment its functionality. If you wish to those functions, you go to an app store and download it. Similarly, when you open RStudio, some base packages are already available that allow you to use a variety of functions. Still, we will almost always need to use functions that require third-party packages. Install Packages Two important points to remember throughout this course. First, NEVER install a package from within your R Markdown document (top-left pane). Instead, ALWAYS install a package using the console pane (bottom-left). Installing packages is one of the very few instances in which we use the console pane. Second, you only have to install a package once. If you install a package, you do not need to install it again unless you want or need to update it. To install R packages, we type the following generic code into the console pane (bottom-left). install.packages(&quot;name_of_package&quot;) Where \"name_of_package\" is replaced with the actual name of the R package we want to install. If we run this code, it accesses R’s “app store” and downloads the package to your computer. Again, you only need to install a R package once. The package is saved on your computer where R can find it. Remember to include quotes around the name of the R package when installing it. Exercise 5 We will almost always need to use a package called tidyverse. In your console pane (bottom-left) of RStudio, type install.packages(“tidyverse”), then click Enter on your keyboard. This will begin the installation. Be sure to monitor the console pane while tidyverse installs. RStudio may ask you some Yes/No questions during the installation. If you receive the following question: Do you want to install from sources the package which needs compilation?, respond with typing no and hit Enter. For any other questions, respond with typing Yes then click Enter. You can install multiple R packages at once. You will need numerous R packages to complete this course. Below is a list of all R packages used in this course except for tidyverse, which you just installed and do not need to install again. install.packages(c(&quot;knitr&quot;, &quot;arsenal&quot;, &quot;forecast&quot;, &quot;fpp2&quot;, &quot;lubridate&quot;, &quot;readxl&quot;, &quot;plm&quot;, &quot;broom&quot;, &quot;car&quot;, &quot;gvlma&quot;, &quot;gapminder&quot;, &quot;Ecdat&quot;, &quot;carData&quot;, &quot;openintro&quot;, &quot;fivethirtyeight&quot;, &quot;moderndive&quot;, &quot;DAAG&quot;, &quot;Stat2Data&quot;, &quot;data.table&quot;, &quot;rmarkdown&quot;, &quot;tinytex&quot;, &quot;DiagrammeR&quot;, &quot;jtools&quot;, &quot;blogdown&quot;, &quot;stargazer&quot;)) Complete the following exercise to download all of the packages needed for the course. Exercise 6 In your console pane (bottom-left) of RStudio, copy-and-paste the above code, then click Enter. This will begin the installation. Again, RStudio may ask you some Yes/No questions during the installation. If you receive the following question: Do you want to install from sources the package which needs compilation?, respond with typing no and hit Enter. For any other questions, respond with typing Yes then click Enter. Load Packages When an application is installed on your phone, you still have to launch it (i.e. click on it) to use it. The same goes for using a R package. Each time you launch RStudio, you need to load the package(s) that contain the functions you plan to use. Closing RStudio is like shutting off your phone. When you open your phone, an application isn’t running in the background unless you previously launched it. Therefore, you should load needed R packages each time you open RStudio. And because you want your code to work each time you or someone else tries to run it, you should include this in your R Markdown document that you save. This is different than installing a package, which you should do in the console, not the saved document. Remember, you have to load a package every time you restart RStudio. Therefore, type the code that loads a package in the R Markdown document that you save. The following generic code is loads a package. library(name_of_package) Do not use quotes around the name of the R package when loading it. Quotes are only needed to install, not load. Exercise 7 Near the top of your document, you should see a code chunk named setup containing the code knitr::opts_chunk$set(echo = TRUE). Start a new line inside of this code chunk. Type library(tidyverse) on the new line. Next, run this code chunk either by clicking on the little sideways arrow at the right of the code chunk, or by using the keyboard shortcut Cmd+Enter on Mac or Ctrl+Enter or Window+Enter on PC. This shortcut only executes the line of code on which your cursor (the blinking vertical line) is. The console pane (bottom-left) will provide information about the loading. Remember, you must load a package before using any functions included in that package or else you will receive the following error message, Error: could not find function. I will tell you what packages are needed to complete all assignments, but remember that you need to install and load the package to use it. 1.4 Save and Upload Save your document once more either by knitting again or like any other software – clicking on the floppy disk icon in the menu at the top or using the menu bar File -&gt; Save As. Then upload your .Rmd document to eLC. Once you upload, answers will become available on the Welcome module of eLC. 1.5 Additional Resources There are many resources that provide orientations to R. Below are two that I consider particularly helpful and accessible. Chapters 1-3 of Getting Used to R, RStudio, and R Markdown BasicBasics of RYouWithMe by R-Ladies Sydney "],["2-r-data.html", " 2 R Data 2.1 Learning Objectives 2.2 Set-up 2.3 Viewing data sets 2.4 Glimpse Data 2.5 Variable Types in R 2.6 Save and Upload", " 2 R Data 2.1 Learning Objectives In this chapter, you will learn how to: Examine datasets to determine their dimensions, unit of analysis, and structure Examine variables to determine their type 2.2 Set-up To complete this chapter, you need to Start a R Markdown document. Keep the YAML and delete the default content in the body except for the first code chunk named setup containing the code knitr::opts_chunk$set(echo = TRUE). Use the code below to load the required packages. Start a new line inside of the setup code chunk you did not delete, then paste the code. Run this code chunk. library(tidyverse) library(carData) #if this fails, you need to install the package (see RChapter 16) Now that you have successfully loaded the two packages, you have access to the functions they contain as well as any data sets they contain. Many packages include data sets used for demonstrating and learning certain functions work. Before starting the exercises, go to the Packages tab in the bottom-right pane. Find carData in the list and click on the name. This should take you to the Help tab, which will contain the documentation for carData. This page serves as a directory to all of the data sets that come loaded with the package. We will be examining some of these data sets. If you want or need to learn more about a particular data set, you can click on its name in this list. 2.3 Viewing data sets Perhaps one of the first obstacles to using R is that you do not constantly stare at a spreadsheet, creating somewhat of a disconnect between what you do to data and seeing it done. While you do not need to view a spreadsheet to understand what you are doing to the data, it is often helpful to examine a data set in spreadsheet form when initially trying to understand its contents. You can always examine a data set in spreadsheet form any time you need to. The following information and exercises walk you through how to do so. Your environment pane (top-right) will list all data sets that have been imported during your current R session. We have not imported any data (we will learn how in a different assignment). Therefore, our environment is empty. This will become much clearer later, but any time there is a data set in your environment that you want to view in spreadsheet form, click on its name and a new tab will appear in the top-left pane that displays the data set. Since we loaded carData we do have access to numerous data sets but are not immediately available in the environment pane. To view a data set included in a package, we want to copy it over to our environment. Exercise 1: Start a new code chunk below the setup code chunk in which you loaded the required packages. To do so, you can use the keyboard shortcuts Cmd+Option+I on Mac or Ctrl+Option+I on PC, or use insert code drop down menu along the top of your R Markdown pane that has a plus sign over a square with a c inside of it. If using the menu, click on the R option. Exercise 2 In the code chunk you just started, paste and run the code provided below, which saves a data set named Arrests contained within the carData package to your environment pane in the top-right. arrests &lt;- Arrests Exercise 3 Click on arrests in your environment pane. A new tab should appear that shows the spreadsheet for arrests. If you find yourself needing to see a spreadsheet, use this point-and-click method. Now that we can view data, let’s practice applying some of the concepts you read about in the Chapter 2. The arrests data set contains information on arrests for possession of small amounts of marijuana in the city of Toronto. Exercise 4 Based on what you see in the spreadsheet of arrests, what is the structure of this data set? What is the unit of analysis? Remember to write your answer outside of a code chunk. Let’s examine a new data set called Florida which contains county voting results for the 2000 presidential election. Exercise 5 Follow the same process used for viewing the Arrests data set to view the Florida data set. What is the structure and unit of analysis of the Florida data set? Exercise 6 Do the same thing one more time with a data set named USPop. What is its structure and unit of analysis? 2.4 Glimpse Data A data set can contain far too many variables (columns) and observations (rows) to effectively scroll through in a spreadsheet. The glimpse function generates a compact printout providing key information about a data set. The general code is as follows: glimpse(dataset) where we replace dataset with the name of the dataset. Exercise 7 In an existing or new code chunk, use glimpse on arrests. Notice that the results show you the dimensions of the dataset–the number of rows (observation) and columns (variables). Next, it provides a vertical list of variables, with several of their values listed horizontally. This allows us to easily view a very wide dataset with many variables and export the information to a document. Exercise 8 Having now examined arrests as a spreadsheet and using glimpse, what type are the following variables based on what you read in Chapter 2 (numerical vs. categorical; continuous, discrete, ordinal, nominal)? year age sex 2.5 Variable Types in R The column immediately to the right of the variable name in the glimpse printout is also informative. It tells you how each variable is stored in R. A variable can be stored in several ways: Integers: commonly used for discrete variables Doubles/numerics: commonly used for continuous variabls but can also store discrete variables Logicals: commonly used for categorical variables that are binary (i.e. 1 or 0). In R, logicals are assigned TRUE, if equal to 1, or FALSE, if equal to 0. Factors: commonly used for categorical variables. Factors can store categorical variables with any number of levels. Therefore, a binary variable can be stored as a factor instead of a logical if you want the variable to be assigned different values like “Yes” or “No.” Characters: commonly used for strings of text that don’t fit the other storage types well, such as open-ended responses in a survey. However, any variable can be stored as a character. A numerical variable can be stored as a character and R will not recognize its values as numbers. Exercise 9 Are the variables listed exercise 8 stored in R in a way that aligns with their type you identified in exercise 8? Variables will not always be stored in R the way they should. Sometimes we have to tell R how to store a variable based on our own understanding of their type. This skill will be covered later. 2.6 Save and Upload Change the title in the YAML to “R Chapter 17”. Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["3-r-missing-data.html", " 3 R Missing Data 3.1 Learning Objectives 3.2 Set-up 3.3 Data 3.4 Checking for missing data 3.5 Counting missing values 3.6 Bypassing missing values 3.7 Drop all missing cases 3.8 Save and Upload", " 3 R Missing Data 3.1 Learning Objectives In this chapter, you will learn how to: Determine if a dataset has missing values Determine which variables in a dataset have missing values and how many values are missing Run functions on variables that have missing values Replace all missing values with a non-missing value, such as 0, if doing so is advisable 3.2 Set-up To complete this chapter, you need to Start a R Markdown file, keeping the YAML and deleting the default content Change the YAML to the following: --- title: &#39;R Chapter 18&#39; author: &#39;Your name&#39; output: html_document --- Load the following packages library(tidyverse) library(carData) 3.3 Data We will use the SLID data from the carData package to learn how to deal with missing data. Per its documentation, “The SLID data frame has 7425 rows and 5 columns. The data are from the 1994 wave of the Canadian Survey of Labour and Income Dynamics, for the province of Ontario. There are missing data, particularly for wages.” As is always the case when we begin working with new data, we want to get a sense of what it contains. Exercise 1: Use glimpse to examine SLID. This is a moderately large dataset with 7,425 observations. Obviously, it would be terribly inefficient to look for missing values manually by scrolling through a spreadsheet of this size. We can already see from the glimpse results that wages has missing values given the NAs. 3.4 Checking for missing data If it is not immediately obvious that a dataset contains missing values, we can tell R to check if an entire dataset has any missing data using the following function anyNA(dataset) where we replace dataset with the name of the dataset. If the dataset has at least one missing value, then anyNA will return TRUE. Exercise 2: Use anyNA to confirm SLID has missing values. The anyNA hasn’t told us anything we didn’t already know given the obvious NAs present in wages. Next, we may want to know which variables have missing values. To determine which variables have missing values, we want to run anyNA repeatedly for each variable in our dataset. To run any function repeatedly on each row or column of a dataset, we can use the following function: apply(dataset, 1 (for rows, or) 2 (for columns), function) where we replace dataset with the name of our dataset, include either 1 or 2, and replace function with the name of the function we want to repeat. Exercise 3: Use apply to run the anyNA function repeatedly on each column. Your results should tell you that wages, education, and language contain missing values. 3.5 Counting missing values Once we know a variable has missing values, we typically want to know how many values are missing or what percentage of total observations are missing for that variable. The is.na function tests every value of a variable for whether it is missing. If a value is NA, is.na returns TRUE. To illustrate, the below code assigns a series of ten values to v, five of which are missing. This v object is no different from a variable in a dataset. Then, using the is.na function on v will return a list of TRUE/FALSE values accordingly. v &lt;- c(NA, 5, NA, 4, 10, 11, NA, NA, 1, NA) is.na(v) [1] TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE TRUE Recall in Chapter 2 that the logical value of TRUE equals 1 in R, while FALSE equals 0. This means we can do math on TRUE/FALSE values just like we would if they were coded as 1/0. If is.na gives us TRUE for every NA, then adding all the TRUEs will give us the total count of missing values. To sum all the values of any variable, we can use the sum function sum(is.na(v)) [1] 5 The result tells us 5 of the 10 values in v are missing. We can easily determine that 50% of the data for v is missing. But what if we have some denominator that is not as easy as 10? We can quickly to determine the percent of missing values by taking the average of TRUEs and FALSEs from the is.na function because the average sums the values of the variable and divides by the number of values. We take the average of the is.na function using the mean function. Since each TRUE value is equal to 1, adding up all the TRUEs will equal 5, which is then divided by the total number of values, 10, giving us 0.5 or 50%. Whenever we have a dummy 1/0 variable, the average of that variable is the percentage of observations equal to 1. In this case, 1 represents missing, but it could represent anything. mean(is.na(v)) [1] 0.5 As expected, we get 0.5 or 50%. Building from this example, we can quantify the total and percent of missing values for wages like so sum(is.na(SLID$wages)) [1] 3278 mean(is.na(SLID$wages)) [1] 0.4414815 Wages is missing 3,278 observations, or about 44% of all observations. Exercise 4: Use the sum and mean function on is.na to determine the count and percent of missing values for the education and language variables. If we had, say, 10 variables with missing values, the process above would be rather tedious. Like before, we can tell R to repeatedly quantify missing values for each variable using a slightly different function: sapply(SLID, function(x) sum(is.na(x))) wages education age sex language 3278 249 0 0 121 sapply(SLID, function(x) mean(is.na(x))) wages education age sex language 0.44148148 0.03353535 0.00000000 0.00000000 0.01629630 3.6 Bypassing missing values Many functions that execute some kind of computation (e.g. sum, average) do not work if you execute them on variables that contain missing values. This is deliberate so users are notified of missing values. For instance, below I try to calculate average years of education. mean(SLID$education) [1] NA In order to have functions bypass missing values, we have to include the na.rm=TRUE option that tells R to skip NAs. mean(SLID$education, na.rm = TRUE) [1] 12.49608 Since education is missing only 3% of its values, this is probably a good approximation of what the average would be if there were no missing values. Exercise 5: Compute the average for wages. It is unclear what to do with average wages since almost half of its values are missing. At the very least, we can report something like, “Only 56% of respondents reported a wage. Of those who reported a wage, the average equals $15.55.” 3.7 Drop all missing cases First, we should always be careful when dropping data, as it could change our analysis and mislead a reader. Always ask yourself why a variable might be missing values and whether it matters to the conclusions you plan to make from the values that are not missing. If you do choose to remove observations that have missing values, always be transparent by stating how many observations from the total were removed due to missing data. Suppose we want to remove all observations from SLID with a missing value for any variable. That is, we want to purge SLID of all missing values, perhaps so we don’t have to keep including na.rm=TRUE in all of our functions. To remove all missing values, we can use the na.omit function like so: dataset_nomissing &lt;- na.omit(dataset) where we create a new dataset indicating we’ve replaced the missing values (we don’t want to overwrite the original data). Inside the na.omit function, we include the name of the dataset. Exercise 6: Create a new dataset SLID_nomissing that removes all missing values. Then calculate the average education on this new dataset without including na.rm=TRUE. Is the average education the same as in SLID? 3.8 Save and Upload Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["4-r-description.html", " 4 R Description 4.1 Learning Objectives 4.2 Set-up 4.3 Introduction 4.4 Quick Stat Computation 4.5 Summary Table 4.6 Correlation Coefficient 4.7 Save and Upload", " 4 R Description 4.1 Learning Objectives In this chapter, you will learn how to: Calculate descriptive statistics individually Automate a professional-quality table of descriptive statistics 4.2 Set-up To complete this chapter, you need to Start a R Markdown document Change the YAML to the following: --- title: &#39;R Chapter 19&#39; author: &#39;Your name&#39; output: html_document: theme: spacelab df_print: paged --- Load the following packages library(tidyverse) library(arsenal) library(knitr) library(carData) 4.3 Introduction Summary statistics tables are ubiquitous in reports and studies. Usually a dataset involves numerous variables that would require too many visualizations, though we should still consider visualizations for the most important variables. A standard table of summary stats provides readers a reference for key measures pertaining to all our variables in a fairly compact form. In this chapter, we set out to summarize variables within the States dataset of the carData package. Exercise 1: Use the glimpse function to examine the States dataset. States is a cross-section of the 50 states and D.C. containing education and related statistics. Be sure to skim the documentation for States to understand each variable. You can do that by clicking on the carData package under the Packages tab then clicking on the States link. 4.4 Quick Stat Computation Instead of producing a full table of descriptive statistics, sometimes we just want to know one or two descriptive measures of a single variable. Because R can hold many datasets/objects at once, we need to tell it which dataset/object to apply a given function. We have had to do this many times already. Similarly, if we want R to apply a function to a specific variable within a dataset, we need to tell which variable in which dataset. This is done using the $ operator. Below are all of the useful descriptive measures of center and spread applied to the pay (i.e. average teacher’s salary in 1,000s) variable in the States dataset. Note that the $ operator tells R to apply the function to a specific variable within a dataset. mean(States$pay) [1] 30.94118 median(States$pay) [1] 30 sd(States$pay) [1] 5.308151 IQR(States$pay) [1] 6 range(States$pay) [1] 22 43 Exercise 2: Calculate the average and standard deviation for state spending on public education in $1000s per student. 4.5 Summary Table Summary tables come in many styles, so there is no way to cover everything. In most cases, a summary table includes the following descriptive measures depending on the type of variable: Numerical variables Mean Standard deviation Minimum Maximum Categorical variables Counts for each level, and/or Percentages for each level If a variable is skewed, then it may be wise to replace the mean and standard deviation with the median and IQR. We will learn how to do this. 4.5.1 Quick Table Sometimes we do not want to print a fancy table. Rather, we may want to quickly see a full set of descriptive statistics for ourselves that our reader will never see. This can be done using the summary function on our dataset like so: summary(gapminder) country continent year lifeExp Afghanistan: 12 Africa :624 Min. :1952 Min. :23.60 Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.20 Algeria : 12 Asia :396 Median :1980 Median :60.71 Angola : 12 Europe :360 Mean :1980 Mean :59.47 Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.85 Australia : 12 Max. :2007 Max. :82.60 (Other) :1632 pop gdpPercap Min. :6.001e+04 Min. : 241.2 1st Qu.:2.794e+06 1st Qu.: 1202.1 Median :7.024e+06 Median : 3531.8 Mean :2.960e+07 Mean : 7215.3 3rd Qu.:1.959e+07 3rd Qu.: 9325.5 Max. :1.319e+09 Max. :113523.1 We would almost certainly want to suppress this code and output (i.e. include=FALSE code chunk option) if preparing a report for an external audience. Note that for the categorical variables, country and continent, summary provides the count of observations instead of measures of center or spread. Exercise 3: Generate a quick table of descriptive statistics for all of the variables in States. Suppress the code and output. 4.5.2 Using Arsenal Due to the many styles of summary tables, there are numerous R packages designed to produce summary tables. The best R package in terms of quickly getting the information to a nicely formatted table of which I am aware is Arsenal. Therefore, we will learn how to use Arsenal. I will demonstrate Arsenal using the gapminder dataset. Then, I will ask you to replicate those demonstrations using the States data. Producing a summary table with Arsenal involves at least two, probably three, steps. Create a new object containing the summary statistics we want to include in a table Relabel the variables to something appropriate for our audience Generating the summary table based on the new object we just created Here is an example following the steps above using gapminder data without altering any of Aresenal’s default options that we will want to know how to alter in some cases. sum.gapminder &lt;- tableby(~ continent + gdpPercap + lifeExp + pop, data = gapminder) The above code is what actually creates the table I want to export. First, I name the object whatever I want. Then I use the tableby function. We will learn what the tilde, ~, does later. For now, know that it is required. Then, I list the variable I want included in the table, separating each with a plus sign. Lastly, I tell R which dataset to apply this function. labels(sum.gapminder) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) Most datasets do not use variable names that would be appropriate for an external audience. The names in gapminder are not bad; most readers could make sense of what the names imply about the data, but it is simple enough (though tedious) to provide a more polished look. Therefore, in the above code I use the labels function on the sum.gapminder table I just created, then assign each variable I told R to include in the table a label that will replace the name when it prints. summary(sum.gapminder, title = &quot;Summary Stats for Gapminder Data&quot;) Table 4.1: Summary Stats for Gapminder Data Overall (N=1704) Continent    Africa 624 (36.6%)    Americas 300 (17.6%)    Asia 396 (23.2%)    Europe 360 (21.1%)    Oceania 24 (1.4%) GDP Per Capita    Mean (SD) 7215.327 (9857.455)    Range 241.166 - 113523.133 Life Expectancy    Mean (SD) 59.474 (12.917)    Range 23.599 - 82.603 Population    Mean (SD) 29601212.325 (106157896.744)    Range 60011.000 - 1318683096.000 This last line of code actually prints the summary table when I knit my document. The previous two steps (tableby and labels) can be included in the same code chunk, but this third step needs to have its own code chunk because you need to include a specific code chunk option, results='asis', in order for the table to export properly. To be clear, in the top line of a code chunk that contains {r} by default, you need to change it to {r, results='asis', echo=FALSE}. I also include the echo=FALSE option assuming we do not want our reader to see our code. Exercise 4: Replicate the code shown above to create a summary table for the States data using the Arsenal package. Be sure to relabel the variables to something relatively understandable and brief. Labeling is tedious but you only need to do it once. I suggest you knit your document now to see what you just made. In three relatively short bits of code, we already have a decent summary table that would have taken excruciatingly long to input manually. But it can be made better. 4.5.3 Adjustments to Arsenal 4.5.3.1 Decimal digits The biggest aesthetic issue with my table is that it includes so many decimals. None of these variables have such a small range that rounding to integers masks useful information. Obviously, if a variable only ranges between 0 and 1, we would not want to round to an integer. Specifying the number of decimals is quite easy with Arsenal. Because arsenal tries to be as flexible as possible, we have to specify the number of decimals separately for numerical and percentage measures. The following code sets the number of decimals to zero for the gapminder data. sum.gapminder2 &lt;- tableby(~ continent + gdpPercap + lifeExp + pop, data = gapminder, digits = 0, digits.pct = 0) labels(sum.gapminder2) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder2, title = &quot;Summary Stats for Gapminder Data&quot;) Table 4.2: Summary Stats for Gapminder Data Overall (N=1704) Continent    Africa 624 (37%)    Americas 300 (18%)    Asia 396 (23%)    Europe 360 (21%)    Oceania 24 (1%) GDP Per Capita    Mean (SD) 7215 (9857)    Range 241 - 113523 Life Expectancy    Mean (SD) 59 (13)    Range 24 - 83 Population    Mean (SD) 29601212 (106157897)    Range 60011 - 1318683096 Exercise 5: Replicate the code shown above to create a second summary table for the States data with no decimals. Note that you can simply copy-and-paste the labels code. 4.5.3.2 Reporting median and IQR Instead of the mean and standard deviation, we may want to report the median, first quartile, and third quartile for our numerical variables. We can control the descriptive measures using the following code. sum.gapminder3 &lt;- tableby(~ continent + gdpPercap + lifeExp + pop, data = gapminder, digits = 0, digits.pct = 0, numeric.stats = c(&quot;median&quot;, &quot;q1q3&quot;, &quot;range&quot;)) labels(sum.gapminder3) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder3, title = &quot;Summary Stats for Gapminder Data&quot;) Table 4.3: Summary Stats for Gapminder Data Overall (N=1704) Continent    Africa 624 (37%)    Americas 300 (18%)    Asia 396 (23%)    Europe 360 (21%)    Oceania 24 (1%) GDP Per Capita    Median 3532    Q1, Q3 1202, 9325    Range 241 - 113523 Life Expectancy    Median 61    Q1, Q3 48, 71    Range 24 - 83 Population    Median 7023596    Q1, Q3 2793664, 19585222    Range 60011 - 1318683096 Exercise 6: Replicate the code shown above to create a summary table for the States data that reports median and the first and third quartiles. 4.5.3.3 Across groups Finally, instead of reporting summary statistics for the entire sample, we may want to report them separately for each level of a categorical variable. This is a common way to make comparisons. We can have Arsenal report across groups by adding the categorical variable to the left side of the formula in the tableby code. The code below reports the gapminder data across continents. Note that the tilde ~ is used to separate grouping variables on the left side from the variables we wish to summarize on the right side. By default, Arsenal tests for correlations across groups and reports a p-value. This is not a common part of a summary table (at least for fields in which I am familiar), so I turn this feature off with the test = FALSE within the code below. sum.gapminder4 &lt;- tableby(continent ~ gdpPercap + lifeExp + pop, data = gapminder, digits = 0, digits.pct = 0, test = FALSE) labels(sum.gapminder4) &lt;- c(continent = &quot;Continent&quot;, gdpPercap = &quot;GDP Per Capita&quot;, lifeExp = &quot;Life Expectancy&quot;, pop = &quot;Population&quot;) summary(sum.gapminder4, title = &quot;Summary Stats for Gapminder Data&quot;) Table 4.4: Summary Stats for Gapminder Data Africa (N=624) Americas (N=300) Asia (N=396) Europe (N=360) Oceania (N=24) Total (N=1704) GDP Per Capita    Mean (SD) 2194 (2828) 7136 (6397) 7902 (14045) 14469 (9355) 18622 (6359) 7215 (9857)    Range 241 - 21951 1202 - 42952 331 - 113523 974 - 49357 10040 - 34435 241 - 113523 Life Expectancy    Mean (SD) 49 (9) 65 (9) 60 (12) 72 (5) 74 (4) 59 (13)    Range 24 - 76 38 - 81 29 - 83 44 - 82 69 - 81 24 - 83 Population    Mean (SD) 9916003 (15490923) 24504795 (50979430) 77038722 (206885205) 17169765 (20519438) 8874672 (6506342) 29601212 (106157897)    Range 60011 - 135031164 662850 - 301139947 120447 - 1318683096 147962 - 82400996 1994794 - 20434176 60011 - 1318683096 Exercise 7: Replicate the code shown above to create a summary table for the States data that reports across regions. 4.5.4 Export to CSV Knitting your notebook to HTML, Word, or PDF should produce a summary table in the appropriate format. Depending on our or others’ workflow, we may want to export our summary table to CSV in order to open in Excel or other spreadsheet software. Arsenal can easily handle this. To export my gapminder summary to CSV, I need to create a new object that contains the actual summary table. Below, I save the last summary to the object named sum.table. sum.table &lt;- summary(sum.gapminder4, title = &quot;Summary Stats for Gapminder Data&quot;) Next, I need to convert this table into a data frame using the as.data.frame() function like so. sum.table &lt;- as.data.frame(sum.table) Lastly, I just need to save this data frame as a CSV file using the write.csv() function like so. write.csv(sum.table, file = &quot;sumtable.csv&quot;) R will save the CSV file to my project folder. Otherwise, R will save the file to my current working directory. 4.6 Correlation Coefficient As mentioned in Chapter ??, the correlation coefficient quantifies the direction and strength of association between two numerical variables. It is rare to see correlations used in any table. Instead, correlations are typically used as an exploratory tool to inform a more advanced analysis like regression. Nevertheless, we may want to report a specific correlation coefficient in our prose. To calculate the correlation coefficient between two variables, we can use the cor() function like so: cor(gapminder$gdpPercap, gapminder$lifeExp) [1] 0.5837062 where I include two variables from the gapminder dataset. To calculate correlation coefficient between all numerical variables in a dataset, we can simply include the dataset in cor without specifying any variable. Note that I must first remove the variables that are not numeric. gapminder %&gt;% select(-country, -continent) %&gt;% cor() year lifeExp pop gdpPercap year 1.00000000 0.43561122 0.08230808 0.22731807 lifeExp 0.43561122 1.00000000 0.06495537 0.58370622 pop 0.08230808 0.06495537 1.00000000 -0.02559958 gdpPercap 0.22731807 0.58370622 -0.02559958 1.00000000 Exercise 8: Calculate the correlation coefficients between all the variables in States. Which two variables have the stongest correlation? What is the direction? 4.7 Save and Upload Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["5-r-visualization.html", " 5 R Visualization 5.1 Learning Objectives 5.2 Set-up 5.3 Grammar of graphics 5.4 Histogram 5.5 Box plot 5.6 Bar chart 5.7 Scatter plot 5.8 Line graph 5.9 Save and Upload", " 5 R Visualization 5.1 Learning Objectives In this chapter you will learn how to make the following visualizations: Histogram Box plot Bar chart Line graph Scatter plot The code used to make the above visualizations in ?? will be provided and explained. Then, you will be asked to replicate the visualization using different data. 5.2 Set-up To complete this chapter, you need to Start a R Markdown document Change the YAML to the following: --- title: &#39;R Chapter 20&#39; author: &#39;Your name&#39; output: html_document: theme: spacelab df_print: paged toc: true toc_float: toc_collapsed: false fig_width: 6 fig_height: 4 fig_align: &quot;center&quot; --- The newest additions to the YAML header are worth explaining. The toc stands for table of contents, which works really well for HTML output but not so much Word or PDF. Each level of the table of contents is dictated by the headings you include in your document. The arguments beginning with fig dictate the size and alignment of each figure your code produces. You can override these global arguments by including with code chunk options. Load the following packages and data library(tidyverse) library(data.table) # contains fread function to import from URL library(fpp2) countyComplete &lt;- fread(&#39;http://openintro.org/data/tab-delimited/county_complete.txt&#39;) For everything but the line graph, we will use the countyComplete data within the openintro package. This dataset contains 3,143 counties and 53 variables. For the line graph, we will use the prisonLF data within the fpp2 package. This dataset is a quarterly time series of prisoner numbers in Australia from 2005 to 2016, split by sex, state, and legal status. 5.3 Grammar of graphics R uses the grammar of graphics to make visualizations. You need to define three essential elements to produce a graph: data: defines the dataset containing our variable(s) of interest aes: defines the variables used to generate the plot and how they are used geom: defines the kind of plot We plot variables from data to aesthetic attributes of geometric objects. The function we use to do this is called ggplot. The generic code below shows the essential elements that will produce a default plot. We replace data with the name of the dataset. Within the aes parentheses, we tell R to assign one or more variables to a variety of attributes, such as x or y or color. What we include within aes depends on the type of plot we want to make, which is determined by the second line which always includes geom_ followed by the type of plot. ggplot(data, aes(x = variable, ...)) + geom_type() For example, the below code takes the variable gdpPercap from the dataset gapminder and maps it to the x axis aesthetic of a chosen geometric object called a histogram. ggplot(gapminder, aes(x = gdpPercap)) + geom_histogram() Data, aesthetics, and geometries are the essential elements needed to generate a graph. If you tell R these three elements correctly, it will produce a graph. There are additional elements that can be added to make graphs be more effective or look better that will be covered in class. Aesthetics take variables in your data and assign them to attributes that correspond to the geometric object you intend to use. That is, aes and geom work together and must be compatible. For example, you can’t generate a scatter plot–geom_point–if you only define an x aesthetic. You must define an x and y aesthetic for a scatter plot. Below is a list of available aesthetics and what they control: x: x axis y: y axis color: differentiate groups by color; change color of outlines of shapes size: diameter of points based on values of a variable; static size of points or thickness of lines fill: fill a shape with color alpha: transparency linetype: line pattern labels: uses text instead of plot points; adds text to axes shape: differentiate groups by shape of plot points The type of your variable also informs which aesthetic(s) to use. Continuous variables: x and y size fill Categorical variables: labels color and/or fill shape linetype size 5.4 Histogram Here is the code used to make the histogram from Chapter ??. The dataset is named college_grad_students and the variable assigned to the x axis aesthetic is named grad_median, which is the median earnings of full-time employees with various graduate school majors. Then, a geom_histogram is added. The rest of the code inside the histogram parentheses, the labs parentheses (stands for labels), and the theme_classic is optional and used to make the histogram look more polished. ggplot(college_grad_students, aes(x = grad_median)) + geom_histogram(bins = 30, fill = &#39;steelblue&#39;, color = &#39;white&#39;) + labs(x = &#39;Median earnings&#39;, y = &#39;Count of graduate majors&#39;) + theme_classic() Figure 5.1: Histogram of full-time median earnings for different graduate school majors Here is the code for the same histogram without the optional code. This is all that is needed to generate a histogram. In general, all plots require very little code if we do not care what they look like. ggplot(college_grad_students, aes(x = grad_median)) + geom_histogram() Exercise 1: Add a heading # Histogram. In the countyComplete dataset, there is a variable named bachelors_2010 that measures the percent of the county population with a bachelor’s degree between 2006-2010. Suppose we want to visualize the distribution of bachelors with a histogram. Generate a simple, default histogram (no optional code unless you want to add it). ggplot(countyComplete, aes(x = bachelors_2010)) + geom_histogram() 5.5 Box plot Below is the code used for the box plot in Chapter ??. Like the histogram, a box plot visualizes the distribution of one variable but uses descriptive measures median, first and third quartile, and identifies extreme values. Therefore, we only need to tell R which variable to assign to the either the x or y axis. Note that I assign grad_median to the y axis so that the box plot is vertical, which is merely a stylistic choice. Assigning grad_median to the x axis would make the box plot horizontal. Then, geom_boxplot is used to tell R to make a boxplot from grad_median. ggplot(college_grad_students, aes(y = grad_median)) + geom_boxplot(fill = &#39;steelblue&#39;) + labs(y = &#39;Median earnings&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) Figure 5.2: Box plot of full-time median earnings for different graduate school majors Again, all the code after geom_boxplot is optional and was used to make the box plot look more polished. The fill = 'steelblue' changes the color of the box, labs is used to help the reader understand what grad_median measures, theme_classic is one of several themes built within R that changes the look of a plot, and the code inside the theme function removes all of the ink related to the x axis due to it being unnecessary. The theme function allows you to control every element of a plot. Unique themes can be created and saved for replication, saving time and avoiding errors. Again, if we do not care how the box plot looks, all we need to make the plot is shown in the code below. ggplot(college_grad_students, aes(y = grad_median)) + geom_boxplot() Exercise 2: Add another heading # Boxplot. Generate a simple, default box plot for bachelors_2010 (no optional code unless you want to add it). 5.6 Bar chart There are two functions that make bar graphs: geom_bar and geom_col. Recall that a bar chart is used to show counts or proportions of levels within a categorical variable. In Chapter ??, a bar chart was used to show the counts and proportions of graduate majors defined as having either high or low unemployment. The table below shows a few rows and variables from the data. Note that these data are disaggregated with respect to the count of majors belonging to high or low unemployment. That is, we need our bar chart to count the number of rows with “high” and “low” in the unemp_cat column. In this case, we should use geom_bar. major grad_total grad_unemployment_rate unemp_cat grad_median Public Administration 42661 0.059 high 75000 Political Science And Government 695725 0.039 low 92000 International Relations 69355 0.045 low 86000 Public Policy 15284 0.031 low 89000 Below is the code used to generate the side-by-side or dodged bar chart from Chapter ??. Note the use of geom_bar, which requires either an x or y aesthetic to be defined. Here, I assign the categorical variable unemp_cat to the x aesthetic, making the bar chart vertical. Assigning it to the y aesthetic would make the bar chart horizontal. Again, all the code past geom_bar is optional. ggplot(gradschool, aes(x = unemp_cat)) + geom_bar(fill = &#39;steelblue&#39;) + labs(y = &#39;Count of degrees&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) Here is what the bar chart looks like without the optional code. ggplot(gradschool, aes(x = unemp_cat)) + geom_bar() Below is the code used to generate the stacked bar chart showing counts, Figure ??. The code within aes is less intuitive. Since geom_bar requires an x or y aesthetic to be defined, I have to assign x to nothing via the blank quotation marks. The fill = unemp_cat tell R to stack the bar chart, filling the bar with the counts of high and low. ggplot(gradschool, aes(x = &quot;&quot;, fill = unemp_cat)) + geom_bar() + labs(y = &#39;Count of degrees&#39;, fill = &#39;Unemployment&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) Below is what the stacked bar chart looks like by default. ggplot(gradschool, aes(x = &quot;&quot;, fill = unemp_cat)) + geom_bar() Lastly, the code below is used to show proportions rather than counts. The only substantive difference between this code and the code above is the use of position='fill' within the geom_bar function. This tells R to show proportions. ggplot(gradschool, aes(x = &quot;&quot;, fill = unemp_cat)) + geom_bar(position = &#39;fill&#39;) + labs(y = &#39;Proportion of degrees&#39;, fill = &#39;Unemployment&#39;) + theme_classic() + theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) Exercise 3: Add a heading # Bar Chart. Suppose we want to visualize how many counties each state has. That is, we want to count how many rows belong to each state in the countyComplete data using a bar chart. Generate a bar chart that achieves this. Choose the type of bar chart you deem best. When should we use geom_col instead? When our counts are already aggregated in our data. Refer back to the table above. Note that grad_total and grad_median contain the count of graduates within each major and their median pay, respectively. Therefore, we do not need R to count the number of rows in our data, but rather report each number already included in the data. The geom_col function takes these counts and visualizes them using a bar (or column) chart. The below code shows how to generate Figure ??, which visualized the median pay for the four majors in the data related to those offered by SPIA. Median pay is simply a number in the data that does not need counting, thus the code uses geom_col, which requires both an x and y aesthetic to be defined. To allow room for the long names of each major, I made the bar chart horizontal by assigning major to the y aesthetic. Each bar visually represents the numbers for grad_median in the above table. There is a new piece of code in the below chunk that can be used to reorder bars in ascending or descending order, which is generally preferred over random order of peaks and valleys. The reorder(major, -grad_median) code tells R to reorder the majors in the plot in ascending because of the minus sign; removing it would reverse the order to descending. ggplot(gradschool_spia, aes(y = reorder(major, -grad_median), x = grad_median)) + geom_col(fill = &#39;steelblue&#39;) + theme_classic() + labs(x = &#39;Median pay&#39;) + theme( axis.title.y = element_blank(), axis.line.y = element_blank(), axis.ticks.y = element_blank() ) Below is what the bar chart looks like without the optional code. ggplot(gradschool_spia, aes(y = major, x = grad_median)) + geom_col() 5.7 Scatter plot The code below shows how the scatter plot in Chapter ?? was generated. This scatter plot actually contains two geometric objects. The geom_point function generates the scatter plot, and the geom_smooth function generates the regression/trend line. Note the assignment of x and y aesthetics that every scatter plot requires. Everything beyond geom_point is optional. ggplot(gradschool, aes(x = grad_median, y = grad_total)) + geom_point(color = &#39;steelblue&#39;, size = 2) + geom_smooth(method = &#39;lm&#39;, se = FALSE, linetype = &#39;dashed&#39;, color = &#39;black&#39;) + scale_y_log10(label=scales::comma_format()) + labs(y = &#39;Total degrees&#39;, x = &#39;Median pay&#39;) + theme_minimal() Below is the scatter plot without optional code. ggplot(gradschool, aes(x = grad_median, y = grad_total)) + geom_point() Exercise 4: Add a heading # Scatterplot. Pick two variables in the countyComplete data and plot their relationship using a simple scatter plot. 5.8 Line graph Line graphs are best for visualizing variables over time (i.e. time series). The prisonLF data separate prisoner counts by male vs. female, remanded vs. sentenced, and state. Therefore, there are four times series for each state. The code below generates a line graph for the time series of female prisoners who were sentenced in each Australia state. Note how this code is different from the code before because I need to manipulate it before creating the plot. Specifically, I pipe the prisonLF data into the filter verb, which keeps only the rows with Female and Sentenced. Then, I pipe that result into the typical ggplot. However, I do not need to specify the dataset because it is already being piped into ggplot. Therefore, ggplot only needs aes and geom to be defined. Time t is assigned to the x aesthetic, count is assigned to the y aesthetic, and state is assigned to the color aesthetic. The color aesthetic is a common way to plot multiple groups. It also provides a legend by default. The geom_line function generates a line graph. prisonLF %&gt;% filter(gender == &#39;Female&#39; &amp; legal == &#39;Sentenced&#39;) %&gt;% ggplot(aes(x = t, y = count, color = state)) + geom_line() + labs(color = &#39;State&#39;, y = &#39;Female Sentenced Prisoners&#39;, x = &#39;&#39;) + theme_minimal() Exercise 5: Add a heading # Linegraph. Create a line graph for male prisoners who were sentenced by state. 5.9 Save and Upload Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["6-r-regression.html", " 6 R Regression 6.1 Learning Objectives 6.2 Set-up 6.3 Running Regression 6.4 Reporting Regression Estimates 6.5 Save and Upload", " 6 R Regression 6.1 Learning Objectives In this chapter, you will learn how to: Run the following regression models: Continuous outcome and explanatory variable(s) Categorical explanatory variable Binary categorical (i.e. dummy) outcome (linear probability model) Interaction of two explanatory variables Generate tables of key results 6.2 Set-up To complete this chapter, you need to Start a R Markdown document Change the YAML to at least the following. Feel free to add arguments. --- title: &#39;R Chapter 21&#39; author: &#39;Your name&#39; output: html_document: theme: spacelab df_print: paged --- Load the following packages library(tidyverse) library(moderndive) library(Stat2Data) library(carData) We will use the TeenPregnancy dataset within the Stat2Data package and the Salaries dataset within the carData package. While data in most packages is available in the background once the package is loaded, we need to manually load datasets from Stat2Data in order to use them. Run the following code, and the dataset should show up in your Environment pane in the top-right. data(&quot;TeenPregnancy&quot;) Be sure to view the documentation for these data by clicking on the package name under the packages tab in the bottom-right pane, then click on the dataset. 6.3 Running Regression Chapters ?? and ?? cover the following regression models: Simple linear regression with two numerical variables Multiple linear regression with all numerical variables Including a categorical explanatory variable (parallel slopes) Regression with a categorical explanatory interacted with a numerical variable Regression with a binary categorical outcome (linear probability model) While our interpretation of results may need to adjust according to which of the above regression models we run, the code to run a linear regression is the same regardless of the number and type of explanatory variables we include and whether the outcome variable is continuous or a binary categorical variable. With the exception of including an interaction, running the regression models listed above can be done with the same code structure shown below. 6.3.1 General Syntax new_object_name &lt;- lm(outcome ~ exp_1 + exp_2 + ... + exp_k, data = name_of_dataset) We name a new object that will hold the results of our regression lm is the function for linear regression (acronym for linear model) We replace outcome with the name of our outcome variable that should be either numerical or binary The tilde ~ separates the outcome on the left-hand side of the regression equation from the explanatory variables on the right-hand side We replace the exp_1 to exp_k with the names of however many explanatory variables we wish to include, each separated by a plus sign + We replace name_of_dataset with the name of the dataset that contains the variables for the regression model. 6.3.2 Continuous outcome and continuous or categorical explanatory variables Recall the following multiple regression model from Chapter ??. \\[\\begin{equation} FedSpend = \\beta_0 + \\beta_1Poverty + \\beta_2HomeOwn + \\beta_3Income + \\epsilon \\tag{6.1} \\end{equation}\\] I ran this regression using the following code: load(&#39;data/county_complete.rda&#39;) selcounty &lt;- county_complete %&gt;% select(name, state, fed_spend = fed_spending_2009, poverty = poverty_2010, homeownership = homeownership_2010, income = median_household_income_2010, pop2010) %&gt;% mutate(fed_spend = fed_spend/pop2010) %&gt;% filter(fed_spend &lt; 50) %&gt;% select(-pop2010) fedpov2 &lt;- lm(fed_spend ~ poverty + homeownership + income, data = selcounty) That’s all there is to it. I named the model fedpov2 to remind myself it was the second model I ran to examine the relationship between federal spending and poverty rate. Note that the code within the lm function mimics Equation (6.1). No matter if the explanatory variables happen to be numerical or categorical, the regression works the same in R. Lastly, I did some behind-the-scenes cleaning of the original county data discussed in Chapter ?? and named it selcounty. Therefore, I told R to use that dataset when running the regression. TeenPregnancy is a dataset with 50 observations on the following 4 variables. State State abbreviation CivilWar Role in Civil War (B=border, C=Confederate, O=other, or U=union) Church Percentage who attended church in previous week (from a state survey) Teen Number of pregnancies per 1,000 teenage girls in state Exercise 1: Suppose we want to use the TeenPregnancy dataset to examine whether state teen pregnancy rates are associated with church attendance and a state’s role in the Civil War, which is a categorical variable with four levels (admittedly an odd variable to include but let’s think of it as a proxy for region). The model would be represented using the following formula: \\[\\begin{equation} Teen = \\beta_0 + \\beta_1Church + \\beta_2CivilWar + \\epsilon \\end{equation}\\] Run this regression model. 6.3.3 Interactions Though we only cover interacting a numerical variable with a categorical variable in this course, we can interact two variables of any type using the same code. In theory, we can interact more than two variables. In any case, an interaction requires us to multiply the variables within the lm function. Recall the regression model from Chapter ?? where mrall is traffic fatality rate, vmiles is the average miles driven, and jaild is whether a state imposes mandatory jail for drunk driving. \\[\\begin{equation} mrall = \\beta_0 + \\beta_1 vmiles + \\beta_2 jaild + \\beta_3 vmiles \\times jaild + \\epsilon \\end{equation}\\] I ran this regression using the following code load(&#39;data/trdeath.RData&#39;) interactmod &lt;- lm(mrall ~ vmiles + jaild + vmiles*jaild, data = trdeath) Note that the only difference from the code in the previous example is vmiles*jaild, which tells R to interact the two variables by multiplying them together. Once again, the code reflects the equation. Salaries is a dataset with 397 observations recording rank (AsstProf, AssocProf, Prof), discipline (A = theoretical, B = applied), years since their Ph.D., years of experience, sex, and salary. Exercise 2: Suppose we want to use the Salaries dataset to examine whether professor salary is associated with their sex and how long they have worked at the institution. Furthermore, suppose we theorize that the association between salary and how long they have worked at the insitution differs by sex, thus warranting an interaction. Therefore, we have the following model: \\[\\begin{equation} salary = \\beta_0 + \\beta_1sex + \\beta_2yrs.service + \\beta_3 sex \\times yrs.service + \\epsilon \\end{equation}\\] Run this regression model. 6.3.4 Dummy outcome While a regression with a dummy variable as the outcome does not require any special coding, we do need to make sure the dummy variable is coded as 1/0 in the data. Sometimes a dummy variable will be coded like this already in which case we don’t need to do anything to run the regression. Other times, the dummy variable will be coded using text like “yes” and “no” or “Male” and “Female” in the case of a dummy variable for sex. Recall in Table ?? the coding for jaild is yes/no. Also recall the regression equation (??) for the linear probability model example restated below: \\[Pr(jaild=1)=\\beta_0+\\beta_1vmiles+\\beta_2region+\\epsilon\\] I ran this regression using the following code: trdeath2 &lt;- trdeath %&gt;% mutate(jaild = if_else(jaild == &#39;yes&#39;, 1, 0)) lpm_mod &lt;- lm(jaild ~ mrall + region, data = trdeath2) But this won’t work if we include jaild in our regression code without recoding it to 1/0. This can be done using the following code. trdeath2 &lt;- trdeath %&gt;% mutate(jaild = if_else(jaild == &#39;yes&#39;, 1, 0)) This code creates a new dataset named trdeath2 which is a copy of the trdeath dataset except for changing the values for jaild using the mutate verb. Inside mutate, I name the “new” variable jaild, which overwrites the existing jaild variable based on what follows the equal sign. The if_else function can be used for a variety of purposes, but it is the simplest way to recode a dummy variable in text to 1/0. The first argument is the conditional. Observations that meet this conditional receive the second argument, while observations that do not receive the third argument. Using natural language, I’m telling R, “If jaild equals”yes”, then code it as 1 or else code it as 0.” Exercise 3: Let’s keep using this Salaries data. Suppose we wanted to predict discipline, which again is coded as A = theoretical or B = applied. Suppose we wanted to predict discipline using the following model: \\[\\begin{equation} Discipline = \\beta_0 + \\beta_1Sex + \\beta_2YrsSincePhD + \\epsilon \\end{equation}\\] Run this regression model. 6.4 Reporting Regression Estimates This section presents two ways to obtain results after running a regression. The first uses functions that load with the moderndive package and the second uses functions that load with R by default (i.e. Base R). The moderndive functions are somewhat more intuitive and produce results that look nicer, but the base R functions are more robust to any variety of regression model. 6.4.1 Moderndive To get a standard table of regression estimates using moderndive, we can use the get_regression_table function on our saved regression model results like so get_regression_table(fedpov2) term estimate std_error statistic p_value lower_ci upper_ci intercept 23.519 1.333 17.645 0.000 20.905 26.132 poverty -0.056 0.021 -2.674 0.008 -0.097 -0.015 homeownership -0.126 0.012 -10.736 0.000 -0.149 -0.103 income 0.000 0.000 -7.723 0.000 0.000 0.000 To get goodness-of-fit measures, we can use the get_regression_summaries function like so get_regression_summaries(fedpov2) r_squared adj_r_squared mse rmse sigma statistic p_value df nobs 0.064 0.063 20.72216 4.55216 4.555 71.055 0 3 3123 and if I only want the R-squared, Adjusted R-squared, and RMSE from this table, I can add the select function to the above code chunk like so get_regression_summaries(fedpov2) %&gt;% select(r_squared, adj_r_squared, rmse) r_squared adj_r_squared rmse 0.064 0.063 4.55216 Exercise 4: Produce a standard table of regression estimates and goodness-of-fit measures for each of your three regression models using the moderndive functions. 6.4.2 Base R A comprehensive set of regression results can be obtained using the summary function on our regression model like so summary(fedpov2) Call: lm(formula = fed_spend ~ poverty + homeownership + income, data = selcounty) Residuals: Min 1Q Median 3Q Max -9.463 -2.502 -1.007 1.015 39.327 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.352e+01 1.333e+00 17.645 &lt; 2e-16 *** poverty -5.597e-02 2.093e-02 -2.674 0.00752 ** homeownership -1.258e-01 1.172e-02 -10.736 &lt; 2e-16 *** income -8.593e-05 1.113e-05 -7.723 1.52e-14 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.555 on 3119 degrees of freedom Multiple R-squared: 0.06397, Adjusted R-squared: 0.06307 F-statistic: 71.06 on 3 and 3119 DF, p-value: &lt; 2.2e-16 which gives us most of the information from get_regression_table except for the confidence intervals. The summary function also reports the R-squared and Adjusted R-squared at the bottom. The esidual standard error at the bottom is not exactly the same as the RMSE above–it is actually equal to sigma in the full table from get_regression_summaries–but you can treat them the same. To get the confidence intervals, we can use the confint function like so, confint(fedpov2) 2.5 % 97.5 % (Intercept) 20.9051552232 2.613204e+01 poverty -0.0970010683 -1.493693e-02 homeownership -0.1488027358 -1.028456e-01 income -0.0001077445 -6.411382e-05 which uses the 95% confidence level by default. Exercise 5: Produce a comprehensive set of results for each of your three regression models using the Base R functions. 6.5 Save and Upload Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["7-r-nonlinear-regression.html", " 7 R Nonlinear Regression 7.1 Learning Objectives 7.2 Set-up 7.3 Quadratic term 7.4 Log Transformation 7.5 Save and Upload", " 7 R Nonlinear Regression 7.1 Learning Objectives In this chapter, you will learn how to: Run a regression with a quadratic term Run a regression with log transformations 7.2 Set-up To complete this chapter, you need to Start a R Markdown document Change the YAML to at least the following. Feel free to add arguments. --- title: &#39;R Chapter 22&#39; author: &#39;Your name&#39; output: html_document: theme: spacelab df_print: paged --- Load the following packages library(tidyverse) library(moderndive) library(carData) We will use the Mroz dataset within the carData package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 7.3 Quadratic term Recall the below regression model from Chapter ?? that includes a squared term for Age, which allows our regression line to change directions once as Age changes. We included this term because Figure ?? suggested wages initially increase with age, then decreases. \\[\\begin{equation} Wage = \\beta_0 + \\beta_1Age + \\beta_2Age^2 + \\beta_3Educ + \\epsilon \\end{equation}\\] The below code demonstrates how to include a quadratic term within the lm function. quad_mod &lt;- lm(Wage ~ Age + I(Age^2) + Educ, data = wages) In this case, the code reflects the equation only somewhat; the I() is necessary to tell R that Age^2 is the squared version of Age. Otherwise, R would not recognize Age^2 in the data, thus excluding it from the regression. Now we can obtain results in the usual manner. get_regression_table(quad_mod) term estimate std_error statistic p_value lower_ci upper_ci intercept -22.722 3.023 -7.517 0 -28.742 -16.701 Age 1.350 0.134 10.077 0 1.083 1.617 I(Age^2) -0.013 0.001 -9.840 0 -0.016 -0.011 Educ 1.254 0.090 13.990 0 1.075 1.432 We need to alter the Mroz data slightly before running a regression. Run the following code that creates a new variable that equals 1 if lfp equals “yes” and 0 if lfp equals “no.” This is necessary because our outcome variable–even though categorical–must be represented numerically in order for the regression to work. my_Mroz &lt;- Mroz %&gt;% mutate(lfp_numeric = if_else(lfp == &quot;yes&quot;, 1, 0)) Exercise 1: Suppose we want to examine factors that explain whether married women participate in the labor force, which is a binary outcome. We use the following model: \\[\\begin{equation} lfp = \\beta_0 + \\beta_1k5 + \\beta_2age + \\beta_3age^2 + \\beta_4wc + \\beta_5lwg + \\beta_6inc + \\epsilon \\end{equation}\\] Run this regression model and obtain the results. 7.4 Log Transformation In Chapter ??, the following log-log regression model was run. \\[\\begin{equation} ln(LifeExp)=\\beta_0 + \\beta_1ln(GDPpercap) + \\beta_2Continent + \\epsilon \\end{equation}\\] The below code demonstrates how to transform a variable into its natural log within the lm function. loglog &lt;- lm(log(lifeExp) ~ log(gdpPercap) + continent, data = gapminder) Note that all we need to do is place the appropriate variables within the log() function, which R interprets as the natural log. This temporarily transforms the variables; it does not create new variables in the dataset equal to the natural log of the variables. Now we can obtain results in the usual manner. get_regression_table(loglog) term estimate std_error statistic p_value lower_ci upper_ci intercept 3.062 0.026 117.692 0 3.011 3.113 log(gdpPercap) 0.112 0.004 31.843 0 0.105 0.119 continent: Americas 0.133 0.011 12.519 0 0.112 0.154 continent: Asia 0.110 0.009 12.037 0 0.092 0.128 continent: Europe 0.166 0.012 14.357 0 0.143 0.189 continent: Oceania 0.152 0.029 5.187 0 0.095 0.210 Exercise 2: Suppose we decide we want to use the natural log of family income exclusive of wife’s income, inc, resulting in the following model \\[\\begin{equation} lfp = \\beta_0 + \\beta_1k5 + \\beta_2age + \\beta_3age^2 + \\beta_4wc + \\beta_5lwg + \\beta_6ln(inc) + \\epsilon \\end{equation}\\] Run this regression model and obtain the results. 7.5 Save and Upload Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["8-r-evaluations.html", " 8 R Evaluations 8.1 Learning Outcomes 8.2 Set-up 8.3 Chi-square test 8.4 T-tests 8.5 Save and Upload", " 8 R Evaluations 8.1 Learning Outcomes In this chapter, you will learn how to: Conduct a chi-square test Conduct an independent t-test 8.2 Set-up To complete this chapter, you need to Start a R Markdown document Change the YAML to at least the following. Feel free to add arguments. --- title: &#39;R Chapter 23&#39; author: &#39;Your name&#39; output: html_document: theme: spacelab df_print: paged --- Load the following packages library(tidyverse) library(carData) library(MASS) For the chi-square test, we will use the MplsStops dataset within the carData package. For the t-tests, we will use the UScrime dataset within the MASS package. Be sure to view the documentation for these data in the Help tab of the bottom-right pane by typing the name of the dataset in the search bar. 8.3 Chi-square test A chi-square test, like the one demonstrated in Chapter ??, requires two steps: Create a cross-tabulation table using the table function Run the chi-square on the cross-tabulation using the chisq.test function 8.3.1 Cross-tab Below is the code used to produce the cross-tab from Chapter ??. I save the new table as polltable. Using the table function, I tell R which two variables from the poll dataset to cross-tabulate. The $ is how we identify a specific variable within a dataset. The levels of the first variable, response, will be tabulated by row, while the frequency of the levels of the second variable, party, will be tabulated by column. polltable &lt;- table(poll$response, poll$party) Table 8.1: Response by political party conservative liberal moderate Apply for citizenship 57 101 120 Guest worker 121 28 113 Leave the country 179 45 126 Not sure 15 1 4 8.3.2 Run chi-square Now that we have a cross-tabulation table, we can run the chi-square test. The code below demonstrates how. chisq.test(immigration_poll) Warning in chisq.test(immigration_poll): Chi-squared approximation may be incorrect Pearson&#39;s Chi-squared test data: immigration_poll X-squared = 111.22, df = 6, p-value &lt; 2.2e-16 Then, it is simply a matter of interpreting the results. Exercise 1: Using the MplsStops data, suppose we wanted to test whether receiving a citation after being stopped by the police, citationIssued, is independent of race. Both are nominal variables, so a chi-square test can be used. Run this chi-square test. Exercise 2: Are the two variables independent? Why? 8.4 T-tests To reiterate, if the two groups in a t-test are comprised of different subjects, we use an independent t-test. If they are comprised of the same subjects, then we use a dependent t-test. 8.4.1 Independent t-test The code below demonstrates how the independent t-test from Chapter ?? was conducted. The t.test function works a lot like the lm function in that the outcome is entered first, then we input the variable that identifies the groups, which is essentially an explanatory variable. The two variables are separated by ~. Then, we tell R which dataset to use, which is called jobtrain in this case. t.test(earnings ~ treatment, data = jobtrain) Welch Two Sample t-test data: earnings by treatment t = -1.1921, df = 275.58, p-value = 0.2342 alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 95 percent confidence interval: -11629.708 2856.939 sample estimates: mean in group 0 mean in group 1 21645.10 26031.49 Exercise 3: Using the UScrimes data, suppose we wanted to test whether the probability of imprisonment, Prob, is independent of between Southern and non-Southern states, So. The outcome is numerical and the explanatory is nominal. Therefore, a t-test can be used. Run this t-test. Exercise 4: Is there an association between the two variables? Why? 8.4.2 Dependent t-test To conduct a dependent t-test, add the option paired=TRUE inside the t.test code like so t.test(earnings ~ treatment, data = jobtrain, paired = TRUE) However, this code will not work because the number of observations in the treatment and control groups are not equal. If we truly had a paired sample where the same subjects measured twice, then we should have the same number of observations in both groups. 8.5 Save and Upload Knit your Rmd to save it and check for errors. If you are satisfied with your work, upload to eLC. Once you upload, answers will become available for download. "],["A-coding-tips.html", "A Coding Tips A.1 Keyboard Shortcuts A.2 Specifying Datasets and Variables A.3 Assignment and Pipe Operators", " A Coding Tips A.1 Keyboard Shortcuts There are three things you will do often in this course for which there are keyboard shortcuts that will save you time and energy over the long run. Insert a code chunk: Cmd+Opt+I on Mac or Ctrl+Alt+I on Windows Run the current line or selection of code: Cmd+Return on Mac or Ctrl+Enter on Windows Knit document: Cmd+Shift+K on Mac or Ctrl+Shift+K on Windows There are many more keyboard shortcuts. Accessing keyboard shortcuts has a keyboard shortcut! It is Opt+Shift+K on Mac or Alt+Shift+K on Windows. A.2 Specifying Datasets and Variables A.2.1 Datasets R can store multiple datasets or objects at a time for you to work with. Therefore, you must tell R the dataset on which to run a function. Suppose I want R to provide a quick preview of a dataset named gapminder. I can use the glimpse() function for this like so: glimpse(gapminder) Rows: 1,704 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … Failure to tell R a dataset that has been loaded in your environment will result in an error. For example, suppose I misspell the dataset so that R looks for an object that does not exist. glimpse(gasminder) Error in glimpse(gasminder) : object &#39;gasminder&#39; not found A.2.2 Variables Some functions pertain not to an entire dataset but to a specific variable within a dataset. Suppose I wanted to compute an average using the mean() function. Only specifying the dataset results in an error because the mean of a dataset makes no sense. mean(gapminder) argument is not numeric or logical: returning NA[1] NA Instead, I need to specify a variable for which I want the average. To specify a variable within a dataset, we use the $ operator. Suppose I want to compute the average life expectancy, lifeExp, in the gapminder dataset. mean(gapminder$lifeExp) [1] 59.47444 A.3 Assignment and Pipe Operators A.3.1 Assignment Operator Whenever we run a function we have the option of saving the result as a new object to reference for future use. To save a the result of anything to a new object, we use the assignment operator, &lt;-. Whatever happens on the right side of &lt;- is assigned to whatever name we give the new object on the left side. I just computed average life expectancy. Suppose I want to save that result. In that case, I would use the following code: avg_lifeExp &lt;- mean(gapminder$lifeExp) This avg_lifeExp will now show up in my environment pane (top-right) as a single value. This works not just for specific values but for anything we want to save to use later in our code. Note that R did not print out the result like it did when I ran mean(gapminder$lifeExp) above. This is because R assumes I do not want the printout because I am saving it. If I want R to print the result, I can simply run the object name like so avg_lifeExp [1] 59.47444 or I can wrap the code originally assigning the new object in parentheses (avg_lifeExp &lt;- mean(gapminder$lifeExp)) [1] 59.47444 You will use the assignment operator often in this course. The keyboard shortcut for it is Opt + -, that is option and the minus sign key on Mac, or Alt + - on Windows. A.3.2 Pipe Operator We do not have to do one thing at a time in a code chunk, nor do we need to save a new object with each function we apply to an existing object. Suppose I want a dataset that includes only 1952 as well as country and life expectancy. I could use code like so (this involves functions you may not know yet): gap_1952 &lt;- filter(gapminder, year == 1952) gap_1952_lifeExp &lt;- select(gap_1952, country, lifeExp) The first line of the above code uses the filter function to save a new object named gap_1952 that contains gapminder observations only for which year equals 1952. The second line of the above code uses the select function to save a new object named gap_1952_lifeExp that includes only the country and lifeExp variables from the gap_1952 dataset. That code includes unnecessary intermediate object/dataset. It is also difficult to read and follow because you have to track which datasets are used in each step. The pipe operator makes this sort of iterative process much easier to code and read. gap_1952_lifeExp &lt;- gapminder %&gt;% filter(year == 1952) %&gt;% select(country, lifeExp) The pipe operator pipes/feeds the result of what precedes it to the next line and so on. In the code above, I start by naming a new object, gap_1952_lifeExp. This new object is determined by taking the gapminder dataset, then piping it to the filter function that keeps observations for which year equals 1952, then piping the result of that – which is equivalent to the intermediate gap_1952 dataset from before – to the select function that keeps only the country and lifeExp variables. Now we can compute the average life expectancy in 1952 like so: mean(gap_1952_lifeExp$lifeExp) [1] 49.05762 Note how much easier it is to read the code using the pipe operator compared to the code that does not. With the pipe operator, you can read code from left-to-right to understand what is being done. Also, recall that you must specify the dataset for any function. If you use the pipe operator, you do not need to specify the dataset in every function included in the pipe because you will have already fed the dataset to the next line containing the function. For example, the filter function by default requires us to specify the object like so filter(gapminder, year == 1952) This is the same code that produced the above intermediate dataset, gap_1952. But note how when using pipes, one does not need to specify the dataset within the filter function. This is because the pipe operator already feeds the gapminder dataset to the filter function. gap_1952 &lt;- gapminder %&gt;% filter(year == 1952) Forgetting to exclude the dataset from a function when using the pipe operator will result in an error. gap_1952 &lt;- gapminder %&gt;% filter(gapminder, year == 1952) Error: Problem with `filter()` input `..1`. x Input `..1$country` must be a logical vector, not a factor&lt;bf6dc&gt;. The keyboard shortcut for the pipe operator is Cmd+Shift+M for Mac or Ctrl+Shift+M for Windows. "],["B-appendixB.html", "B Wrangle and Tidy Reference B.1 Cheatsheets B.2 Wrangle Verbs B.3 Tidy Verbs", " B Wrangle and Tidy Reference Unless data are already perfectly prepared, the most time consuming part of data analysis is wrangling and tidying data. It is impossible to cover all scenarios one may encounter when preparing raw data for an analysis. Even for advanced users of R, it is not uncommon to search for an unknown solution to a new problem via the web, texts, or manuals. Attempting to memorize the plethora of functions in R that could serve as solutions would quickly result in diminishing returns. Instead, it is more realistic to obtain enough familiarity with basic wrangle and tidy problems and solutions that one knows how and where to effectively search for the solution. B.1 Cheatsheets RStudio provides numerous cheatsheets to help R users reference commonly used and helpful functions. Below is a list of cheatsheets that pertain to wrangling and tidying. This is the most relevant cheatsheet for what you will encounter in the course: Data transformation Others that are less relevant: Factors Working with string variables Dates and times Knowing just a handful of functions can help you make considerable progress in many situations. The remainder of this chapter serves as a sort of cheatsheet for problems you may encounter during the course. Functions are demonstrated using the gapminder data. The tidyverse package is actually a collection of several packages designed to make the wrangle, tidy, and data exploration process as intuitive and consistent as possible. You should almost always load tidyverse, as it contains every function you may need to wrangle and tidy data. library(tidyverse) B.2 Wrangle Verbs filter: extract rows/cases select: extract columns/variables mutate: alter existing variables or create new variables if_else: use a conditional to create a new variable equal to one value if an observation meets the conditional and another value if it does not; often combined with mutate arrange: reorder rows in ascending or descending order of one or more variables slice_head &amp; slice_tail: extract a few rows from the top (head) or bottom (tail) summarize: collapses data into a table of summary statistics group_by: tells R to apply functions to each group separately; common to use with summarize B.2.1 Filter Use filter to extract rows from a dataset. Inversely, one can think of filter as a way to remove rows. However, remember that filter keeps the rows that meet the condition on which you filter. Therefore, you want to use a condition that keeps the rows you want. Note there are 1,704 rows in the gapminder dataset. glimpse(gapminder) Rows: 1,704 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … Suppose I want to keep only countries in Asia. Then: gapminder %&gt;% filter(continent == &#39;Asia&#39;) %&gt;% glimpse() Rows: 396 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … The result is a new dataset with 396 rows. Note the use of double equal signs == to tell R it is a conditional (“if equal to”) rather than setting something equal to something else, which would not make sense in this case. Suppose I want countries in Asia AND in the year 1952. Then: gapminder %&gt;% filter(continent == &#39;Asia&#39; &amp; year == 1952) %&gt;% glimpse() Rows: 33 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Bahrain&quot;, &quot;Bangladesh&quot;, &quot;Cambodia&quot;, &quot;China&quot;,… $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, … $ lifeExp &lt;dbl&gt; 28.801, 50.939, 37.484, 39.417, 44.000, 60.960, 37.373, 37.4… $ pop &lt;int&gt; 8425333, 120447, 46886859, 4693836, 556263527, 2125900, 3720… $ gdpPercap &lt;dbl&gt; 779.4453, 9867.0848, 684.2442, 368.4693, 400.4486, 3054.4212… This results in a new dataset with 33 rows. Note the use of the ampersand &amp; to code the “and” conditional. Suppose I want countries in Asia with a life expectancy less than or equal to 40 in 1952. Then: gapminder %&gt;% filter(continent == &#39;Asia&#39; &amp; year == 1952 &amp; lifeExp &lt;= 40) %&gt;% glimpse() Rows: 10 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Bangladesh&quot;, &quot;Cambodia&quot;, &quot;India&quot;, &quot;Indonesia… $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia $ year &lt;int&gt; 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952 $ lifeExp &lt;dbl&gt; 28.801, 37.484, 39.417, 37.373, 37.468, 36.319, 36.157, 37.5… $ pop &lt;int&gt; 8425333, 46886859, 4693836, 372000000, 82052000, 20092996, 9… $ gdpPercap &lt;dbl&gt; 779.4453, 684.2442, 368.4693, 546.5657, 749.6817, 331.0000, … Suppose I all countries in 1952 except those in Asia. There are a few options to do this. Which option is most efficient depends on the specific case. In this case: Option 1: Using the “or” conditional | (least efficient) gapminder %&gt;% filter(continent == &#39;Africa&#39; | continent == &#39;Americas&#39; | continent == &#39;Europe&#39; | continent == &#39;Oceania&#39;) %&gt;% glimpse() Rows: 1,308 Columns: 6 $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Alba… $ continent &lt;fct&gt; Europe, Europe, Europe, Europe, Europe, Europe, Europe, Euro… $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 72.0… $ pop &lt;int&gt; 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 278009… $ gdpPercap &lt;dbl&gt; 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.004, … Option 2: Using the shortcut %in% for multiple “or” conditionals (moderately efficient) gapminder %&gt;% filter(continent %in% c(&#39;Africa&#39;, &#39;Americas&#39;, &#39;Europe&#39;, &#39;Oceania&#39;)) %&gt;% glimpse() Rows: 1,308 Columns: 6 $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Alba… $ continent &lt;fct&gt; Europe, Europe, Europe, Europe, Europe, Europe, Europe, Euro… $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 72.0… $ pop &lt;int&gt; 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 278009… $ gdpPercap &lt;dbl&gt; 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.004, … Option 3: Use the “not equal to” conditional != (most efficient) gapminder %&gt;% filter(continent != &#39;Asia&#39;) %&gt;% glimpse() Rows: 1,308 Columns: 6 $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Albania&quot;, &quot;Alba… $ continent &lt;fct&gt; Europe, Europe, Europe, Europe, Europe, Europe, Europe, Euro… $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 55.230, 59.280, 64.820, 66.220, 67.690, 68.930, 70.420, 72.0… $ pop &lt;int&gt; 1282697, 1476505, 1728137, 1984060, 2263554, 2509048, 278009… $ gdpPercap &lt;dbl&gt; 1601.056, 1942.284, 2312.889, 2760.197, 3313.422, 3533.004, … B.2.2 Select Suppose I want a dataset that contains only country, continent, year, and life expectancy. There are multiple options. Which is more efficient depends on the specific case. In this case: Option 1: List the variables I want to keep (least efficient) gapminder %&gt;% select(country, continent, year, lifeExp) %&gt;% glimpse() Rows: 1,704 Columns: 4 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… Option 2: List the variables I don’t want to keep (moderately efficient) gapminder %&gt;% select(-pop, -gdpPercap) %&gt;% glimpse() Rows: 1,704 Columns: 4 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… Option 3: Use : to specify the range of variables, which only works because the variables I want happen to be stored next to each other (most efficient) gapminder %&gt;% select(country:lifeExp) %&gt;% glimpse() Rows: 1,704 Columns: 4 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… B.2.3 Mutate The mutate function allows you to mutate your dataset by either changing an existing variable or creating a new one. Suppose I wanted to change GDP per capita so that it is expressed in thousands of dollars instead of dollars. Then: gapminder %&gt;% mutate(gdpPercap = gdpPercap/1000) %&gt;% glimpse() Rows: 1,704 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… $ gdpPercap &lt;dbl&gt; 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.7399811, 0.786… Note that I use the name of an existing variable on the left-hand side of the equation. This overwrites the data according to the function I have specified. You can scroll up to previous glimpses to confirm that gdpPercap has indeed been divided by 1,000. Suppose I wanted a new variable that measures total GDP to have in addition to GDP per capita expressed in thousands. Since GDP per capita equals GDP divided by population, I can simply use the inverse of this calculation. Thus: gapminder %&gt;% mutate(gdpPercap = gdpPercap/1000, gdp = gdpPercap*pop) %&gt;% glimpse() Rows: 1,704 Columns: 7 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… $ gdpPercap &lt;dbl&gt; 0.7794453, 0.8208530, 0.8531007, 0.8361971, 0.7399811, 0.786… $ gdp &lt;dbl&gt; 6567086, 7585449, 8758856, 9648014, 9678553, 11697659, 12598… Since mutate applies mathematical functions, there are way too many possible uses to cover here. The second page of the Data transformation cheatsheet lists numerous common functions used with mutate under the “Vector Functions” header. Suppose there are multiple variables you want to mutate using the same function. A common example is when a bunch of variables are expressed as proportions between 0 and 1 when you want them all to be expressed as percentages between 0 and 100. You could use mutate to multiply each variable by 100, but this quickly becomes tedious. Instead, you can use across inside mutate to list the variables you want to mutate, then define the function you want applied to them. For example, suppose I wanted to multiply all of the numerical variables in gapminder by 100 (doesn’t make sense but just go with it). Then: gapminder %&gt;% mutate(across(c(year, lifeExp, pop, gdpPercap), ~ .x*100)) %&gt;% glimpse() Rows: 1,704 Columns: 6 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … $ year &lt;dbl&gt; 195200, 195700, 196200, 196700, 197200, 197700, 198200, 1987… $ lifeExp &lt;dbl&gt; 2880.1, 3033.2, 3199.7, 3402.0, 3608.8, 3843.8, 3985.4, 4082… $ pop &lt;dbl&gt; 842533300, 924093400, 1026708300, 1153796600, 1307946000, 14… $ gdpPercap &lt;dbl&gt; 77944.53, 82085.30, 85310.07, 83619.71, 73998.11, 78611.34, … The first argument c(year, lifeExp, pop, gdpPercap) specifies the variables I want to mutate. The second argument ~ .x*100 tells R the function to use for the mutation. The .x is a generic representation for each of the variables listed in the first argument. B.2.4 Combining filter, select, and mutate You can do some serious wrangling efficiently with filter, select, and mutate. Suppose I wanted a new dataset of GDP (in billions) for European countries in 2007. Recall that the pipe operator, %&gt;%, makes code easier to read and write by feeding the result of what precedes it to the next line that follows and so on. In the code below, I create a new dataset named euro_gdp07 by first taking the gapminder dataset, then feeding it to the filter verb. The result is a dataset that includes only European countries in 2007, but this dataset is not created explicitly. Instead, it is fed to the mutate verb, which adds a variable named gdp_billions. Finally, this dataset is fed to the select verb. Using the glimpse verb we can see the final result. euro_gdp07 &lt;- gapminder %&gt;% filter(continent == &#39;Europe&#39; &amp; year == 2007) %&gt;% mutate(gdp_billions = (gdpPercap*pop)/1000000000) %&gt;% select(country, year, gdp_billions) glimpse(euro_gdp07) Rows: 30 Columns: 3 $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Austria&quot;, &quot;Belgium&quot;, &quot;Bosnia and Herzegovina&quot;… $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200… $ gdp_billions &lt;dbl&gt; 21.376411, 296.229401, 350.141167, 33.897027, 78.213929, … B.2.5 Combining Mutate and If_Else There are two common cases for using the combination of mutate and if_else: Convert the values of a two-level categorical variable (i.e. dummy variable) from text to numerical Convert the values of a numerical variable or categorical variable with more than two levels to a two-level categorical variable In either case, we can choose to create a new variable or overwrite the existing variable we wish to convert. Suppose I want to create a new variable named rich equal to “yes” if a European country has a GDP greater than the average GDP and “no” if their GDP is less than or equal to the average. euro_gdp07 &lt;- euro_gdp07 %&gt;% mutate(rich = if_else(gdp_billions &gt; mean(gdp_billions), &quot;yes&quot;, &quot;no&quot;)) The first line in the above code overwrites the euro_gdp07 dataset by using the same name on the left side of the assignment operator &lt;-. The euro_gdp07 is fed/piped to the mutate verb. Inside mutate, a name a variable rich. Since rich does not currently exist in the euro_gdp07 dataset, a new variable will be added. This new variable named rich is defined using the if_else function. The first argument is the conditional. Here I define the conditional as “if gdp_billions is greater than the mean of gdp_billions”. Observations that meet the conditional you specify receive the second argument. In this case, European countries with a GDP greater than the mean of GDP among all European countries will receive a value equal to “yes”. Observations that do not meet the conditional you specify receive the third argument. In this case, European countries with a GDP less than or equal to the mean of GDP among all European countries will receive a value equal to “no”. glimpse(euro_gdp07) Rows: 30 Columns: 4 $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Austria&quot;, &quot;Belgium&quot;, &quot;Bosnia and Herzegovina&quot;… $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200… $ gdp_billions &lt;dbl&gt; 21.376411, 296.229401, 350.141167, 33.897027, 78.213929, … $ rich &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;ye… Now suppose instead of using text (i.e. string variable) for rich, I want to use a numerical coding of 1/0 where 1 denotes yes/true and 0 no/false. euro_gdp07 &lt;- euro_gdp07 %&gt;% mutate(rich = if_else(rich == &quot;yes&quot;, 1, 0)) Since rich already exists in euro_gdp07, I use the conditional “if rich equals yes.” If it does, the variable is overwritten with the value 1. If it does not, it is overwritten with the value 0. glimpse(euro_gdp07) Rows: 30 Columns: 4 $ country &lt;fct&gt; &quot;Albania&quot;, &quot;Austria&quot;, &quot;Belgium&quot;, &quot;Bosnia and Herzegovina&quot;… $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200… $ gdp_billions &lt;dbl&gt; 21.376411, 296.229401, 350.141167, 33.897027, 78.213929, … $ rich &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, … B.2.6 Arrange The arrange verb is useful if you want to identify cases that have the highest or lowest values for one or more variables. By default, arrange reorders rows in ascending order (i.e. lowest to highest). In the previous glimpse, countries are arranged in alphabetical order. Suppose I wanted them arranged based on GDP. euro_gdp07 %&gt;% arrange(gdp_billions) %&gt;% glimpse() Rows: 30 Columns: 4 $ country &lt;fct&gt; &quot;Montenegro&quot;, &quot;Iceland&quot;, &quot;Albania&quot;, &quot;Bosnia and Herzegovi… $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200… $ gdp_billions &lt;dbl&gt; 6.336476, 10.924102, 21.376411, 33.897027, 51.774743, 65.… $ rich &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … Now we see a few countries with the lowest GDP. If instead I wanted GDP arranged from highest to lowest, then: euro_gdp07 %&gt;% arrange(desc(gdp_billions)) %&gt;% glimpse() Rows: 30 Columns: 4 $ country &lt;fct&gt; &quot;Germany&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Italy&quot;, &quot;Spain&quot;, … $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200… $ gdp_billions &lt;dbl&gt; 2650.87089, 2017.96931, 1861.22794, 1661.26443, 1165.7598… $ rich &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … Now we see some of the wealthiest European countries. B.2.7 Slice_Head/Tail By default, the slice_head and slice_tail verbs extract the top and bottom 6 rows of a dataset, respectively. These verbs are useful if we want to show a reader a sample of the data in a familiar spreadsheet form, which can be useful. Though the output from glimpse is very useful, it does not look good in a report. The slice_head and slice_tail verbs allow us to provide similar information in a much more presentable format. Suppose we wanted to show a reader the three wealthiest and poorest European countries (in absolute terms). We can specify the number of rows slice_head or slice_tail extract using n=#. Thus: euro_gdp07 %&gt;% arrange(desc(gdp_billions)) %&gt;% slice_head(n=3) %&gt;% kable(digits = 0) country year gdp_billions rich Germany 2007 2651 1 United Kingdom 2007 2018 1 France 2007 1861 1 Note the use of kable in the last line. This function from the knitr package is a common way to print nicer looking tables. The digits= inside specifies how many digits to the right of the decimal to include in the table. In this case, I tell R to round to the nearest whole number. euro_gdp07 %&gt;% arrange(gdp_billions) %&gt;% slice_head(n=3) %&gt;% kable(digits = 0) country year gdp_billions rich Montenegro 2007 6 0 Iceland 2007 11 0 Albania 2007 21 0 B.2.8 Summarize Summarize creates a new dataset by collapsing all of the cases of a dataset into one or more summary statistics. It is useful for providing quick summary stat calculations in a somewhat presentable format. I do not recommend using summarize to produce the kind of summary stats table commonly found in reports because it can become tedious and the formatting is not good enough. I recommend using the arsenal package instead. Suppose I wanted to report the average gdpPercap and lifeExp for 2007 in a rough and ready table. Then: gapminder %&gt;% filter(year == 2007) %&gt;% summarize(&#39;Average GDP per capita&#39; = mean(gdpPercap), &#39;Average life expectance&#39; = mean(lifeExp)) %&gt;% kable(digits = 0) Average GDP per capita Average life expectance 11680 67 The summarize verb works with numerous summary functions listed on the second page of the Data transformation cheatsheet under the heading “Summary Functions.” B.2.9 Group_By The group_by verb is most commonly used in tandem with summarize. If instead of calculating a summary stat for the entire dataset, you wanted to calculate the summary stat for each group of a categorical variable separately, use group_by before using summarize. Suppose I wanted average GDP per capita and life expectancy in 2007 for each continent. Then: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(&#39;Average GDP per capita&#39; = mean(gdpPercap), &#39;Average life expectance&#39; = mean(lifeExp)) %&gt;% kable() continent Average GDP per capita Average life expectance Africa 3089.033 54.80604 Americas 11003.032 73.60812 Asia 12473.027 70.72848 Europe 25054.482 77.64860 Oceania 29810.188 80.71950 Pretty powerful! Also, notice how the values in the table are reported to a fairly useless degree of precision because I did not specify digits=0 inside of the kable function. You can also use multiple grouping variables. Suppose I wanted these summary stats for each continent each year since 1997. Then: gapminder %&gt;% filter(year &gt;= 1997) %&gt;% group_by(continent, year) %&gt;% summarize(&#39;Average GDP per capita&#39; = mean(gdpPercap), &#39;Average life expectancy&#39; = mean(lifeExp)) %&gt;% kable(digits=0) continent year Average GDP per capita Average life expectancy Africa 1997 2379 54 Africa 2002 2599 53 Africa 2007 3089 55 Americas 1997 8889 71 Americas 2002 9288 72 Americas 2007 11003 74 Asia 1997 9834 68 Asia 2002 10174 69 Asia 2007 12473 71 Europe 1997 19077 76 Europe 2002 21712 77 Europe 2007 25054 78 Oceania 1997 24024 78 Oceania 2002 26939 80 Oceania 2007 29810 81 B.3 Tidy Verbs As with wrangling, one can encounter numerous different tidying scenarios. However, most of the time tidying involves converting a wide dataset to a long dataset. The most common untidy data one encounters is a time series or panel data where each time period is stored across columns (i.e. wide) rather than down rows (i.e. long). Let’s begin with a simple time series of population taken from the gapminder data. Suppose we downloaded a dataset named uspop for U.S. population. country 1997 2002 2007 United States 272911760 287675526 301139947 We don’t want each year to be a variable. Rather, we want year to be one variable with separate levels/rows for each period. We can achieve this with pivot_longer. uspop %&gt;% pivot_longer(cols = &#39;1997&#39;:&#39;2007&#39;, names_to = &#39;year&#39;, values_to = &#39;pop&#39;) %&gt;% kable(format = &#39;html&#39;) country year pop United States 1997 272911760 United States 2002 287675526 United States 2007 301139947 Note that pivot_longer tries to make the code as intuitive as possible using natural language. First, we tell R which columns to pivot, then we tell R to name the new column ‘year’, then we tell R to name the new column with the values for population ‘pop’. Suppose we encountered a more difficult wide version of the gapminder data named gap_wide shown below. This one has multiple variables listed wide for each year. Tidying gap_wide will take two steps. First, we can separate the variable names pop/lifeExp/gdpPercap from the numeric year into two columns using pivot_longer. This will result in a column that contains all three variables that precede the year and a column that contains year. We will also need to name a third column that will contain the values that the pivoted columns contained. In the code below, I tell R which columns to pivot using cols and to name the two new columns ‘var’ and ‘year’. I use names_sep to tell that each of the columns should be separated using the underscore. Then, I give the new column that will contain the values the generic name ‘value’ since this is an temporary column. gap_long1 &lt;- gap_wide %&gt;% pivot_longer(cols = pop_1997:gdpPercap_2007, names_to = c(&#39;var&#39;, &#39;year&#39;), names_sep = &#39;_&#39;, values_to = &#39;value&#39;) DT::datatable(gap_long1, rownames = FALSE, options = list(pageLength = 5, scrollX=T)) Now we need to convert the var column to wide using pivot_wider. This will create new columns for each of the unique values contained in the ‘var’ column. Since there are three unique values, the result will be three new columns. We also need to specify which column contains the values that will be transferred over to the three new columns. In the code below, I tell R to pivot the ‘var’ column wide and take the values from the ‘value’ column. And voila; we are back to having our original, tidy data. gap_long2 &lt;- gap_long1 %&gt;% pivot_wider(names_from = var, values_from = value) DT::datatable(gap_long2, rownames = FALSE, options = list(pageLength = 5, scrollX=T)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
